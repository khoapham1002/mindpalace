---
layout: post
title: Citizens Behavioral Practice Questions
date: 2025-01-18 19:21 -0800
description: Citizens Behavioral Interview Questions & Categorized Responses
author: khoa_pham
categories: [Programming Hub, Career Preps]
tags: [interview preps]
pin: false
math: true
mermaid: true
toc: true
comments: true
---

## Citizens Bank HireVue

### Category 1: Motivational & Personal Fit Questions  
#### Q1: Why do you want to work for Citizens?  
1. Background -> Citizens' emphasis data-driven decision-making
2. Alignment -> improve customer exp thru data insights
3. Opportunity -> perfect env. to apply analytical skills while learning from professionals
4. Long-term -> contribute financial data strategies, make banking more efficient 

> With my background in data science, SQL, and financial analytics, I see a strong alignment with Citizens' focus on improving customer experience through data insights. I am excited about the opportunity at Citizens because of its strong emphasis on data-driven decision-making in banking.  I also admire how Citizens integrates risk management and technology in banking operations. This internship provides the perfect environment to apply my analytical skills while learning from industry professionals. Long-term, I aspire to contribute to financial data strategies, making banking safer and more efficient.

> I am drawn to Citizens because of its strong commitment to data-driven decision-making in banking. The company's focus on technology, financial analytics, and risk management aligns with my skills in data science, SQL, and financial modeling. Additionally, Citizens fosters an environment of innovation and learning, which excites me as someone eager to apply machine learning and cloud engineering to real-world banking challenges.

#### Q2: Walk me through your resume  
1. Master/Bachelor 
2. Online Retail Forecasting Model -> ETL pipeline (PySpark, SQL)
3. Optimize Banking Data Pipelines,  Credit Card Approvals, Stock Portfolio Analysis
4. Research Exp. predicting mental health risks (Random Forest)
5. Beyond academics (Data Assistant and mentor for Data Science department)

> I'm currently pursuing a Master's in Business Analytics at UMass Amherst, with a strong foundation in Data Science. Throughout my studies, I've worked on various data projects, including an Online Retail Forecasting Model, where I developed an ETL pipeline using PySpark and SQL. Additionally, I optimized banking data pipelines in a project focused on credit card approvals and stock portfolio analysis. I also have research experience predicting mental health risks using Random Forest models. Beyond academics, I've worked as a Data Assistant and mentor, which has strengthened my teamwork and communication skills. Now, I'm eager to apply my analytical and technical expertise at Citizens to solve real-world financial challenges.

#### Q3: Why did you choose this specific division (Data Science/Data Analyst, etc.)?  
1. Uncovering insights from complex datasets to drive decision-making
2. Coursework/Experience -> prepare me to analyze banking data
3. Comfy with financial datasets to help optimize customer solutions and risk assessments

> I chose Data Science because I enjoy uncovering insights from complex datasets to drive decision-making. My coursework in machine learning, regression analysis, and SQL, combined with my experience developing a predictive model for online retail sales, has prepared me to analyze banking data effectively. I am particularly excited about working with large financial datasets to help optimize customer solutions and risk assessments.

### Category 2: Behavioral Questions  
#### Q1: A time when you faced substantial change and how you adapted  
1. **S:** When transitioning from my undergraduate to the accelerated Master's in Business Analytics, I had to quickly adjust to a heavier workload and advanced coursework.
2. **T:** I needed to efficiently manage the transition while maintaining academic performance and continuing extracurricular activities.
3. **A:** I developed a structured study schedule, sought mentorship from professors, and actively engaged in hands-on projects to reinforce learning.
4. **R:** Successfully adapted, maintaining a 3.97 GPA, excelling in advanced courses, and balancing my academic and professional commitments.

Transitioning from my undergraduate to an accelerated Master's program was a significant change, requiring me to adapt quickly to a more rigorous curriculum. I implemented a structured study plan, sought mentorship from professors, and applied hands-on projects to reinforce theoretical learning. This approach allowed me to maintain a 3.97 GPA, manage multiple responsibilities, and successfully navigate the transition.

#### Q2: A time when you achieved a challenging goal, describing strategies, steps, processes, and approaches  
1. **S:** In my Online Retail Data Pipeline and Forecasting Model project, I aimed to optimize an ETL pipeline and improve demand forecasting accuracy.
2. **T:** The goal was to reduce manual errors by 15% and improve forecast accuracy (MAE <10).
3. **A:** Designed scalable ETL workflows using PySpark and SQL; Implemented feature engineering techniques to refine forecasting inputs; Tuned hyper-parameters and tested multiple models to improve accuracy.
4. **R:** Achieved 98% data consistency, reduced errors by 15%, and improved demand prediction accuracy to MAE = 9.41.

One challenging goal I set was improving the demand forecasting accuracy in my Online Retail Data Pipeline project. I first optimized the ETL workflow using PySpark and SQL to ensure clean data. Then, I implemented feature engineering and fine-tuned models to refine predictions. By systematically testing different approaches, I achieved a 15% error reduction and improved model accuracy to MAE = 9.41, surpassing the original goal.

#### Q3: A time you worked effectively with others to solve a problem  
1. **S:** In my Banking and Investment Optimization Framework project, my team needed to improve data quality for financial modeling.
2. **T:** The challenge was to clean and integrate banking data pipelines for better credit card approval accuracy.
3. **A:** Collaborated with team members to identify inconsistencies in the dataset; Assigned tasks based on each person's expertise (feature engineering, model validation, data visualization); Held weekly check-ins to ensure alignment and address bottlenecks.
4. **R:** Improved credit approval accuracy to 79.8% and successfully implemented data enhancements.

While working on a banking data pipeline, my team faced issues with inconsistent financial data affecting model performance. We divided tasks based on expertise—some focused on feature engineering, others on model validation. Through structured collaboration and weekly check-ins, we improved data quality and boosted credit approval accuracy to 79.8%, demonstrating the power of teamwork in solving complex problems.

#### Q4: Describe a hardship you went through and how you overcame it.  
1. **S:** In my Banking and Investment Optimization Framework project, I faced difficulty improving the accuracy of a credit card approval model using logistic regression.
2. **T:** The model initially had low accuracy (below 70%), and I needed to improve it while maintaining interpretability for financial decision-making.
3. **A:** Researched feature engineering techniques, tuned hyper-parameters, and experimented with different data preprocessing methods to enhance model performance.
4. **R:** Improved accuracy to 79.8%, making the model more reliable, and learned valuable lessons in iterative model improvement and debugging.

During my Banking and Investment Optimization Framework project, I built a logistic regression model for credit card approvals. Initially, the model had an accuracy below 70%, making it unreliable for financial decision-making. I identified the issue as insufficient feature engineering, so I researched better ways to preprocess categorical variables and optimize hyper-parameters. After multiple iterations, I improved the model's accuracy to 79.8%. This experience taught me the importance of experimentation, resilience, and continuous learning when solving complex data problems.

#### Q5: A time you had multiple tasks and limited time  
1. **S:** As a Data Assistant at the Center for Teaching and Learning, I managed survey data while balancing coursework and other commitments.
2. **T:** I had to clean and process a large dataset for workshop participation analysis within a tight deadline while preparing for final exams.
3. **A:** Prioritized tasks using a structured to-do list, automated repetitive data-cleaning tasks using Python, and allocated specific time blocks for study and work.
4. **R:** Successfully met both deadlines—delivered a clean dataset ready for analysis and performed well in my exams, improving my time management skills.

While working as a Data Assistant at the Center for Teaching and Learning, I was responsible for cleaning and managing survey data for workshop participation analysis. At the same time, I had final exams approaching, so I had to balance both responsibilities carefully. I created a structured plan, breaking down my tasks and allocating dedicated time slots for each. Additionally, I automated some of the repetitive data-cleaning tasks using Python, which saved me significant time. As a result, I successfully completed my data tasks on schedule and performed well in my exams, improving my time management and efficiency under pressure.

#### Q6: A time you worked in a team against a tight deadline  
1. **S:** University Mental Health Research study — analyzing student mental health risks.
2. **T:** Team had one week to process survey data, train a model, and present findings.
3. **A:** Used Python (Pandas, Scikit-learn) for preprocessing, divided tasks efficiently.
4. **R:** Achieved 75% model accuracy and delivered insights that led to peer-support program recommendations.

In my university research study on mental health, our team had only a week to analyze student survey data and train a predictive model. I took the initiative to clean and preprocess data using Pandas, while my teammates focused on feature engineering and model validation. By maintaining constant communication and tracking progress, we successfully delivered an accurate model (75%) that provided valuable insights for student wellness programs.

### Category 3: Competency & Skills-Based Questions  
#### Q1: A time you learned a new skill  
1. **S:** During my research assistantship in Biomedical Informatics NLP Processing, I needed to work with Hadoop and Spark for large-scale text analysis.
2. **T:** I had limited experience with big data processing and had to quickly learn these tools.
3. **A:** Took an online Hadoop and Spark course; Practiced by implementing MapReduce functions on biomedical text data; Applied my learnings by building inverse index tables to optimize NLP workflows.
4. **R:** Successfully improved text processing speed, enhancing information retrieval efficiency.

As a research assistant, I had to quickly learn Hadoop and Spark for large-scale biomedical text processing. I took an online course, implemented MapReduce functions, and applied the knowledge to build inverse index tables for NLP workflows. This not only enhanced my big data skills but also improved text processing efficiency significantly.

#### Q2: Favorite technical project, describing tech stack and challenges  
1. **S:** My favorite project was the Online Retail Data Pipeline and Forecasting Model, where I built an end-to-end ETL system for sales analysis.
2. **T:** The challenge was ensuring data consistency and improving demand forecasting accuracy.
3. **A:** Used PySpark and SQL to design scalable ETL workflows; Implemented machine learning models (Scikit-learn) to forecast demand; Addressed data inconsistency issues by refining preprocessing techniques.
4. **R:** Improved forecast accuracy (MAE = 9.41) and reduced manual errors by 15%.

My favorite project was the Online Retail Data Pipeline and Forecasting Model, where I built an end-to-end ETL system for sales analysis. The tech stack included PySpark and SQL for scalable ETL workflows and Scikit-learn for machine learning-based demand forecasting. The biggest challenge was handling data inconsistencies, which I resolved by refining preprocessing techniques. Ultimately, the project improved forecast accuracy (MAE = 9.41) and enhanced data reliability.

#### Q3: What kind of datasets have you worked with?  
1. Example: Online retail sales, financial transactions, mental health surveys.
2. Mention: SQL databases (MySQL, Redshift), big data tools (Spark, Hadoop).
3. Highlight: Experience with data aggregation, transformations, and visualization.

For ETL and data analysis, I primarily use Python (Pandas, NumPy) for data wrangling and SQL for querying structured data. I have worked with Apache Spark for processing large datasets efficiently and used AWS services like Redshift and Glue for data storage and transformation. For visualization, I am proficient in Tableau and QuickSight.

### Category 4: Situational & Ethical Questions  
#### Q1: How would you respond if two VP's ask you to complete different tasks, but you only have enough time to complete one?  
1. Prioritize: Assess task urgency and impact.
2. Communicate: Inform both VPs and suggest a timeline.
3. Delegate: If possible, seek help from a colleague.

I would evaluate both tasks' urgency and business impact. Then, I'd communicate with both VPs to set expectations and negotiate a feasible deadline. If necessary, I'd seek assistance from a colleague to ensure all priorities are addressed.

### Category 5: Commercial & Industry Knowledge  
#### Q1: What trends are impacting the financial and banking industry?  
1. Data-Driven Banking: AI and machine learning in fraud detection.
2. Risk Management: Increasing regulatory compliance and cybersecurity.
3. Customer-Centric Tech: Personalized banking services through data analytics.
